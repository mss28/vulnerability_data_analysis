#!/usr/bin/env python
# -*- coding: utf-8 -*-
import re
import time

import requests
from bs4 import BeautifulSoup


def get_details(url):
    base_url = "https://github.com{:s}"
    print('\tGetting detailed advisory from {:s}'.format(url))
    soup = BeautifulSoup(requests.get(url).text, features="lxml")
    element = soup.find("div", {
        "class": "p-3 bg-gray-light border-bottom d-md-flex"
    })

    git_url = element.find("a", {
        "class": "ml-1 text-bold link-gray-dark text-small"
    })
    if git_url is not None:
        git_url = base_url.format(git_url['href'].strip())
    else:
        git_url = ""

    cols = element.find("table").find_all("td")
    affected_version, fixed_version = cols[1].text.strip(), cols[2].text.strip()

    return git_url, affected_version, fixed_version


def get_top_elements(url):
    base_url = "https://github.com{:s}"
    soup = BeautifulSoup(requests.get(url).text, features="lxml")
    elements = soup.find_all("div", {
        "class": "lh-condensed"
    })

    results = []
    for element in elements:
        sev_text = ' '.join(element.find("div", {"class": "mt-1"}).text.strip().replace('\n', '').split())
        severity, lib_name = re.search(r'^.*\((.*)\) was published.* (.*) \(.*$', sev_text).groups()

        data = {
            "lib_name": lib_name,
            "adv_title": element.find("a").text.strip(),
            "severity": severity,
            "published_date": element.find("relative-time")['datetime'],
            "cve_ref": element.find("span", {"class": "text-bold"}).text.strip(),
            "adv_url": base_url.format(element.find("a")['href']).strip(),
        }

        git_url, affected_version, fixed_version = get_details(data['adv_url'])
        data['git_url'] = git_url
        data['affected_version'] = affected_version
        data['fixed_version'] = fixed_version

        results.append(data)

    return results


def write_to_json(data, file_path):
    import json
    with open(file_path, 'w') as fout:
        json.dump(data, fout)


def write_to_csv(data, file_path):
    import csv

    keys = list(data[0].keys())
    with open(file_path, 'w') as output_file:
        dict_writer = csv.DictWriter(output_file, fieldnames=keys)
        dict_writer.writeheader()
        dict_writer.writerows(data)


def run():
    eco_system = "npm"
    max_page = 26
    url_template = "https://github.com/advisories?page={:d}&query=ecosystem%3A{:s}"
    output_path = '../data/npm_advisories'

    page_no = 1
    results = []
    while page_no <= max_page:
        print('Crawling page {:d}'.format(page_no))

        url = url_template.format(page_no, eco_system)
        print('Crawling from {:s}'.format(url))
        results = results + get_top_elements(url)

        page_no += 1
        time.sleep(1)

    write_to_csv(results, output_path + '.csv')
    write_to_json(results, output_path + '.json')

    print('Crawled total {:d} records'.format(len(results)))


if __name__ == "__main__":
    run()
